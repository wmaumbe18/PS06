---
title: "STAT/MATH 495: Problem Set 06"
author: "Wayne Maumbe"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
```
# Collaboration

Please indicate who you collaborated with on this assignment: 

# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
biaspoints <- runif(n_sample)
errorpoints <- runif(n_sample)
fitted<-runif(n_sample)
biaspoints1 <- runif(n_sample)
errorpoints1 <- runif(n_sample)
fitted1<-runif(n_sample)
premse<-runif(n_sample)
premse1<-runif(n_sample)
```


# Computation

```{r warning=FALSE}

for (j in 1:n_sample) { #for each model
  current_sample_at_j <-generate_sample(f,n,sigma)
  out<-lm(y~x,current_sample_at_j) %>% 
    predict(test_set) 
  fitted[j]<-out #points on blue line ie predicted
  out1<-smooth.spline(x=current_sample_at_j$x,y=current_sample_at_j$y, df=99) %>% 
    predict(test_set)
  fitted1[j]<-out1$y[[1]]#points on blue line ie predicted 
  }
 
actual<-rep(f(0.95),n_sample) #red dot ie population
y<-rep(f(0.95),n_sample)+rnorm(n_sample,0,sigma)# black dot ie sample
biaslm<-round((mean(actual-fitted))^2,6)
biassm<-round((mean(actual-fitted1))^2,6)
varfittedlm<-round(var(fitted),6)
varfittedsm<-round(var(fitted1),6)
varerrorlm<-round(var(y),6)
varerrorsm<-round(var(y),6)
mselm<-round(mean((y-fitted)^2),6) #estimate mse
msesm<-round(mean((y-fitted1)^2),6)#estimate mse
amselm<-round(mean((actual-fitted)^2),6)#mse
amsesm<-round(mean((actual-fitted1)^2),6)#mse
sumlm<-round(biaslm+varerrorlm+varfittedlm,6)
sumsm<-round(biassm+varerrorsm+varfittedsm,6)
```

# Tables

As done in Lec 2.7, for both

* An `lm` regression AKA a `smooth.splines(x, y, df=2)` model fit 
* A `smooth.splines(x, y, df=99)` model fit 

output tables comparing:

|Model|  MSE_hat| MSE| bias_squared|   var| irreducible|   sum|
|----:|--------:|----:|------------:|-----:|-----------:|-----:|
|df=2|     `r mselm`|`r amselm` |`r biaslm` |  `r varfittedlm` |    `r varerrorlm`|         `r sumlm` |
|df=99|    `r msesm`| `r amsesm` | `r biassm`  |    `r varfittedsm` |     `r varerrorsm` |         `r sumsm` |

where `sum = bias_squared + var + irreducible`. You can created cleanly formatted tables like the one above by piping a data frame into `knitr::kable(digits=4)`.




# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
2. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
3. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. A possible sanity check is to calculate the $\mbox{MSE}\left[\widehat{f}(0.95)\right]$ and compare it to  `sum = bias_squared + var + irreducible`
2. At the core of this procedure is the procedure above. The difference is that you perform this over all possible values of x by nesting the above for loop to iterate over all values of x.
3. I would use the splines model because though variable its model fits contain the true value. 


